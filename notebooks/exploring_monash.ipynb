{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "# from distutils.util import strtobool\n",
    "import random\n",
    "\n",
    "# from scipy.stats import norm\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# This is just until temporary implementation\n",
    "import os \n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "sys.path.insert(0,cwd + '/../timetransformers')\n",
    "\n",
    "from utils import convert_tsf_to_dataframe\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory data already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Title: COVID-19 Deaths Dataset\n",
      "Keywords: covid19, forecasting, daily series\n",
      "Publication date: 2020-08-21\n",
      "DOI: 10.5281/zenodo.4656009\n",
      "Total size: 0.0 MB\n",
      "\n",
      "Link: https://zenodo.org/record/4656009/files/covid_deaths_dataset.zip   size: 0.0 MB\n",
      "\n",
      "Checksum is correct. (65c233f20dba0b1e63d1fac2448b7c08)\n",
      "All files have been downloaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded covid_deaths_dataset.tsf\n",
      "Archive:  ../data/covid_deaths_dataset.zip\n",
      "  inflating: ../data/covid_deaths_dataset.tsf  \n",
      "Unzipped covid_deaths_dataset.tsf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Title: US Births Dataset\n",
      "Keywords: births, forecasting, daily series\n",
      "Publication date: 2020-06-22\n",
      "DOI: 10.5281/zenodo.4656049\n",
      "Total size: 0.0 MB\n",
      "\n",
      "Link: https://zenodo.org/record/4656049/files/us_births_dataset.zip   size: 0.0 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded us_births_dataset.tsf\n",
      "Archive:  ../data/us_births_dataset.zip\n",
      "  inflating: ../data/us_births_dataset.tsf  \n",
      "Unzipped us_births_dataset.tsf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Checksum is correct. (6680000cb82f7db6ced6b480bde89410)\n",
      "All files have been downloaded.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"../data\"):\n",
    "    # If the directory does not exist, create it\n",
    "    os.makedirs(\"../data\")\n",
    "    print(f\"Directory data created.\")\n",
    "else:\n",
    "    print(f\"Directory data already exists.\")\n",
    "\n",
    "\n",
    "def download_single_datafile(dataset_name, dataset_id):\n",
    "    os.system(f\"zenodo_get {dataset_id}\")\n",
    "    os.system(f\"mv {dataset_name}.zip ../data/{dataset_name}.zip\")\n",
    "    print(f\"Downloaded {dataset_name}.tsf\")\n",
    "\n",
    "    # Unzip the dataset\n",
    "    os.system(f\"unzip -o ../data/{dataset_name}.zip -d ../data/\")\n",
    "    print(f\"Unzipped {dataset_name}.tsf\")\n",
    "\n",
    "    # Remove the zip file\n",
    "    os.system(f\"rm ../data/{dataset_name}.zip\")\n",
    "    os.system(f\"rm md5sums.txt\")\n",
    "\n",
    "    # Convert the tsf file to a pandas dataframe\n",
    "    return convert_tsf_to_dataframe(f\"../data/{dataset_name}.tsf\")[0]\n",
    "\n",
    "\n",
    "def download_data(dataset_dict):\n",
    "    df_list = []\n",
    "    for dataset_name, dataset_id in dataset_dict.items():\n",
    "        df_list.append(download_single_datafile(dataset_name, dataset_id))\n",
    "\n",
    "    return df_list\n",
    "\n",
    "\n",
    "# List of datasets and zenondo IDs\n",
    "datasets_to_load = {\n",
    "    # \"oikolab_weather_dataset\": \"10.5281/zenodo.5184708\",\n",
    "    \"covid_deaths_dataset\": \"10.5281/zenodo.4656009\",\n",
    "    \"us_births_dataset\": \"10.5281/zenodo.4656049\",\n",
    "    # \"solar_4_seconds_dataset\": \"10.5281/zenodo.4656027\",\n",
    "    # \"wind_4_seconds_dataset\": \"10.5281/zenodo.4656032\",\n",
    "    # \"weather_dataset\": \"10.5281/zenodo.4654822\",\n",
    "    # \"hospital_dataset\": \"10.5281/zenodo.4656014\",\n",
    "    # \"electricity_hourly_dataset\": \"10.5281/zenodo.4656140\",\n",
    "    # \"traffic_hourly_dataset\": \"10.5281/zenodo.4656132\",\n",
    "    # \"rideshare_dataset_without_missing_values\": \"10.5281/zenodo.5122232\",\n",
    "    # \"bitcoin_dataset_without_missing_values\": \"10.5281/zenodo.5122101\",\n",
    "    # \"australian_electricity_demand_dataset\": \"10.5281/zenodo.4659727\",\n",
    "    # \"sunspot_dataset_without_missing_values\": \"10.5281/zenodo.4654722\",\n",
    "    # \"london_smart_meters_dataset_with_missing_values\": \"10.5281/zenodo.4656072\",\n",
    "}\n",
    "\n",
    "# \"covid_deaths_dataset\": \"10.5281/zenodo.4656009\",\n",
    "# \"m4_monthly_dataset\": \"10.5281/zenodo.4656480\",\n",
    "\n",
    "dfs = download_data(datasets_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def convert_df_to_numpy(df):\n",
    "#     # Filter the DataFrame\n",
    "#     # data_list = list(df[\"type\"].unique())\n",
    "#     # filtered_df = df[df[\"type\"].isin(data_list)]\n",
    "\n",
    "#     # Select the 'series_value' column from the filtered DataFrame\n",
    "#     selected_series_values = df[\"series_value\"]\n",
    "\n",
    "#     # T_means_ = df[df[\"obs_or_fcst\"] == (\"T_MEAN\", \"PRCP_SUM\")][\"series_value\"]\n",
    "#     selected_series_values = selected_series_values.to_numpy()\n",
    "\n",
    "\n",
    "#     def fill_nans(array):\n",
    "#         array = pd.Series(array)\n",
    "#         array.ffill(inplace=True)  # Forward fill\n",
    "#         array.bfill(inplace=True)\n",
    "#         return array.to_numpy()\n",
    "\n",
    "\n",
    "#     N_data = selected_series_values.shape[0]\n",
    "#     training_data = selected_series_values[0].astype(float)\n",
    "\n",
    "#     for i in range(1, N_data):\n",
    "#         new_data = fill_nans(selected_series_values[i].to_numpy().astype(float))\n",
    "#         training_data = np.vstack((training_data, new_data))\n",
    "\n",
    "#     return training_data\n",
    "\n",
    "# training_data_list = []\n",
    "\n",
    "# for df in dfs:\n",
    "#     training_data_list.append(convert_df_to_numpy(df))\n",
    "\n",
    "\n",
    "def convert_df_to_numpy(dfs):\n",
    "    training_data = []\n",
    "\n",
    "    for df in dfs:\n",
    "        # Select the 'series_value' column from the filtered DataFrame\n",
    "        selected_series_values = df[\"series_value\"]\n",
    "\n",
    "        # T_means_ = df[df[\"obs_or_fcst\"] == (\"T_MEAN\", \"PRCP_SUM\")][\"series_value\"]\n",
    "        selected_series_values = selected_series_values.to_numpy()\n",
    "\n",
    "        def fill_nans(array):\n",
    "            array = pd.Series(array)\n",
    "            array.ffill(inplace=True)  # Forward fill\n",
    "            array.bfill(inplace=True)\n",
    "            return array.to_numpy()\n",
    "\n",
    "        N_data = selected_series_values.shape[0]\n",
    "\n",
    "        for i in range(N_data):\n",
    "            new_data = fill_nans(selected_series_values[i].to_numpy().astype(float))\n",
    "            training_data.append(new_data)\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "training_data_list = convert_df_to_numpy(dfs)\n",
    "len(training_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 91\u001b[0m\n\u001b[1;32m     87\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TimeSeriesDataset(training_data_list, max_seq_length)\n\u001b[1;32m     88\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     92\u001b[0m     train, true, mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     93\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(train[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[118], line 73\u001b[0m, in \u001b[0;36mTimeSeriesDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Concatenate the original tensors with their respective paddings\u001b[39;00m\n\u001b[1;32m     72\u001b[0m train_series \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([train_series, train_series_padding])\n\u001b[0;32m---> 73\u001b[0m true_series \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrue_series\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_series_padding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mask, mask_padding])\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "def normalize_data(data, mean, std):\n",
    "    if mean == 0 and std == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return (data - mean) / std\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, max_sequence_length):\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.means = np.array([np.mean(data[i]) for i in range(len(data))])\n",
    "        self.std = np.array([np.std(data[i]) for i in range(len(data))])\n",
    "        self.data = [\n",
    "            normalize_data(data[i], self.means[i], self.std[i])\n",
    "            for i in range(len(data))\n",
    "        ]\n",
    "        self.probs = (\n",
    "            np.array([len(self.data[i]) for i in range(len(self.data))])\n",
    "            / self.__len__()\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        l = 0\n",
    "        for i in range(len(self.data)):\n",
    "            l += len(self.data[i])\n",
    "        return l\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # I will just randomly select one of the time series\n",
    "        # and then randomly select a subsequence of length max_sequence_length\n",
    "        new_idx = np.random.choice(a=len(self.probs), p=self.probs)\n",
    "        series = self.data[new_idx]\n",
    "        if len(series) > self.max_sequence_length:\n",
    "            # Randomly select a starting point for the sequence\n",
    "            start_index = random.randint(0, len(series) - self.max_sequence_length - 1)\n",
    "\n",
    "            # Slice the series to get a random subsequence of length max_sequence_length\n",
    "            train_series = torch.tensor(\n",
    "                series[start_index : start_index + self.max_sequence_length],\n",
    "                dtype=torch.float32,\n",
    "            ).unsqueeze(-1)\n",
    "            true_series = torch.tensor(\n",
    "                series[start_index + 1 : start_index + self.max_sequence_length + 1],\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "            mask = torch.ones_like(train_series, dtype=torch.bool)\n",
    "\n",
    "            return (\n",
    "                train_series,\n",
    "                true_series,\n",
    "                mask,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            train_series = torch.tensor(\n",
    "                series[: self.max_sequence_length - 1],\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "            true_series = series[1 : 1 + self.max_sequence_length]\n",
    "\n",
    "            mask = torch.ones(len(true_series) - 1, dtype=torch.bool)\n",
    "\n",
    "            # Calculate the number of padding elements needed\n",
    "            padding_length = self.max_sequence_length - len(train_series)\n",
    "\n",
    "            # Create padding tensors\n",
    "            train_series_padding = torch.zeros(padding_length)\n",
    "            true_series_padding = torch.zeros(padding_length)\n",
    "            mask_padding = torch.zeros(padding_length)\n",
    "\n",
    "            # Concatenate the original tensors with their respective paddings\n",
    "            # FIXME: This currently does not work.......\n",
    "            train_series = torch.cat([train_series, train_series_padding])\n",
    "            true_series = torch.cat([true_series, true_series_padding])\n",
    "            mask = torch.cat([mask, mask_padding])\n",
    "\n",
    "            return (\n",
    "                train_series.unsqueeze(-1),\n",
    "                true_series,\n",
    "                mask,\n",
    "            )\n",
    "\n",
    "\n",
    "max_seq_length = 512\n",
    "batch_size = 64\n",
    "\n",
    "dataset = TimeSeriesDataset(training_data_list, max_seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "for batch in dataloader:\n",
    "    train, true, mask = batch\n",
    "    plt.plot(train[0])\n",
    "    plt.plot(true[0])\n",
    "    plt.plot(mask[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
