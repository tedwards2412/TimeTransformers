{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# This is just until temporary implementation\n",
    "import os \n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "sys.path.insert(0,cwd + '/../timetransformers')\n",
    "\n",
    "\n",
    "import Transformer \n",
    "from utils import GradualWarmupScheduler\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(Transformer)\n",
    "\n",
    "output_dim = 2  # To begin with we can use a Gaussian with mean and variance\n",
    "d_model = 32\n",
    "num_heads = 1\n",
    "num_layers = 2\n",
    "d_ff = 32\n",
    "max_seq_length = 200\n",
    "dropout = 0.0\n",
    "num_distribution_layers = 4\n",
    "\n",
    "transformer = Transformer.Decoder_Transformer(\n",
    "    output_dim,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    num_layers,\n",
    "    d_ff,\n",
    "    max_seq_length,\n",
    "    dropout,\n",
    "    num_distribution_layers,\n",
    "    device=device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok lets generate some fake time series data\n",
    "n_data = 10_000\n",
    "t = np.linspace(0, 10, n_data)\n",
    "y1 = np.sin(t) + np.random.normal(0, 0.1, n_data)\n",
    "\n",
    "# Lets plot it\n",
    "plt.scatter(t, y1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to package this up into random subsets of time series data\n",
    "batch_size = 2048\n",
    "\n",
    "\n",
    "def gen_sin_training_data(batch_size):\n",
    "    # Generate random start points\n",
    "    start_points = torch.randint(0, 10, (batch_size,)).to(device)\n",
    "    batch_times = torch.stack(\n",
    "        [\n",
    "            torch.linspace(\n",
    "                start_point.item(), start_point.item() + 20, max_seq_length + 1\n",
    "            ).to(device)\n",
    "            for start_point in start_points\n",
    "        ]\n",
    "    )\n",
    "    y_vals = torch.stack(\n",
    "        [\n",
    "            torch.sin(batch_time).to(device)\n",
    "            + torch.randn(max_seq_length + 1).to(device) * 0.1\n",
    "            for batch_time in batch_times\n",
    "        ]\n",
    "    )\n",
    "    # The v values for the training should be the next data point\n",
    "    return (\n",
    "        batch_times[:, :max_seq_length].unsqueeze(-1),\n",
    "        y_vals[:, :max_seq_length].unsqueeze(-1),\n",
    "        y_vals[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def gen_linear_training_data(batch_size):\n",
    "    # Generate random start points\n",
    "    start_points = torch.randint(0, 10, (batch_size,))\n",
    "    batch_times = torch.stack(\n",
    "        [\n",
    "            torch.linspace(\n",
    "                start_point.item(), start_point.item() + 10, max_seq_length + 1\n",
    "            )\n",
    "            for start_point in start_points\n",
    "        ]\n",
    "    )\n",
    "    y_vals = torch.stack(\n",
    "        [\n",
    "            batch_time + torch.randn(max_seq_length + 1) * 0.1\n",
    "            for batch_time in batch_times\n",
    "        ]\n",
    "    )\n",
    "    # The v values for the training should be the next data point\n",
    "    return (\n",
    "        batch_times[:, :max_seq_length].unsqueeze(-1),\n",
    "        y_vals[:, :max_seq_length].unsqueeze(-1),\n",
    "        y_vals[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "times, y_train, y_true = gen_sin_training_data(batch_size)\n",
    "# times, y_train, y_true = gen_linear_training_data(batch_size)\n",
    "for i in range(10):\n",
    "    plt.plot(times[i].cpu(), y_train[i].cpu() + i)\n",
    "    plt.plot(times[i].cpu(), y_true[i].cpu() + i)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_learning_rate = 1e-3\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=max_learning_rate)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode=\"min\", patience=50, verbose=True, factor=0.5\n",
    "# )\n",
    "Nepochs = 30\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Nepochs, eta_min=1e-6)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer, max_lr=max_learning_rate, total_steps=Nepochs\n",
    "# )\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=Nepochs, eta_min=1e-6\n",
    ")\n",
    "scheduler = GradualWarmupScheduler(\n",
    "    optimizer,\n",
    "    total_warmup_steps=4,\n",
    "    after_scheduler=cosine_scheduler,\n",
    ")\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for rnd in range(Nepochs):\n",
    "    batch_size = 256\n",
    "    with tqdm.trange(100) as progress:\n",
    "        for epoch in progress:\n",
    "            times, y_train, y_true = gen_sin_training_data(batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            output = transformer(y_train)\n",
    "            loss = transformer.Gaussian_loss(output, y_true)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            progress.set_postfix(ordered_dict={\"loss: \": loss.item()})\n",
    "\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_lr()[0]\n",
    "    print(f\"Learning Rate = {current_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "output = transformer(y_train)\n",
    "plt.plot(y_true[i, :].cpu())\n",
    "mean = output[i, :, 0].detach().cpu()\n",
    "std = torch.sqrt(torch.nn.functional.softplus(output[i, :, 1].detach().cpu()))\n",
    "plt.plot(mean)\n",
    "plt.fill_between(np.arange(mean.shape[0]), mean - std, mean + std, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIT(transformer_pred, y_true):\n",
    "    mean = transformer_pred[:, :, 0].cpu().detach().numpy()\n",
    "    var = torch.nn.functional.softplus(transformer_pred[:, :, 1])\n",
    "    std = np.sqrt(var.cpu().detach().numpy())\n",
    "\n",
    "    U = norm.cdf(\n",
    "        y_true.cpu().detach().numpy(),\n",
    "        loc=mean,\n",
    "        scale=std,\n",
    "    )\n",
    "    return U\n",
    "\n",
    "\n",
    "u = PIT(output, y_true)\n",
    "\n",
    "for i in range(10):\n",
    "    plt.hist(u[i], alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(Transformer)\n",
    "Nlines = 50\n",
    "rand_idx = torch.randint(0, y_train.shape[0], (1,))[0]\n",
    "transformer.eval()\n",
    "\n",
    "for i in range(Nlines):\n",
    "    # Generate the next n_sequence elements\n",
    "    n_sequence = 500\n",
    "    generated_sequence = []\n",
    "\n",
    "    # Initial input for the model\n",
    "    current_input = y_train[rand_idx].unsqueeze(0)\n",
    "\n",
    "    sequence = transformer.generate(current_input, n_sequence)\n",
    "    # sequence = generate(current_input, n_sequence)\n",
    "\n",
    "    if i == 0:\n",
    "        sequence_average = sequence\n",
    "    else:\n",
    "        sequence_average += sequence\n",
    "    plt.plot(\n",
    "        np.arange(y_train[rand_idx].shape[0], y_train[0].shape[0] + sequence.shape[1]),\n",
    "        sequence[0, :].detach().cpu(),\n",
    "        alpha=0.3,\n",
    "        color=\"grey\",\n",
    "    )\n",
    "\n",
    "plt.plot(\n",
    "    np.arange(\n",
    "        y_train[rand_idx].shape[0], y_train[rand_idx].shape[0] + sequence.shape[1]\n",
    "    ),\n",
    "    sequence_average[0, :].detach().cpu() / Nlines,\n",
    "    color=\"k\",\n",
    "    ls=\"--\",\n",
    ")\n",
    "plt.plot(np.arange(0, y_train[rand_idx].shape[0]), y_train[rand_idx].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, y_train[0].shape[0]), y_train[0].cpu())\n",
    "plt.plot(\n",
    "    np.arange(y_train[0].shape[0], y_train[0].shape[0] + sequence.shape[1]),\n",
    "    sequence[0, :].detach().cpu(),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
